# Baseline Subset Experiment Configuration for Yeast Dataset
# Establishes baseline data efficiency curves without active learning

experiment:
  name: "baseline_subsets_yeast"
  description: "Train DREAM-RNN on random subsets of yeast promoter data"
  random_seed: 42

# Dataset configuration
data:
  dataset_name: "yeast"
  data_path: "./data/yeast"
  
  # Yeast-specific settings
  sequence_length: 110   # Most common promoter length in dataset (padded/truncated)
  input_channels: 4     # ACGT only (no singleton flag)
  
  # Subset fractions to test
  subset_fractions: [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]
  
  # Data loading
  batch_size: 1024
  num_workers: 0  # Set to 0 to avoid PyTorch 2.4.1 DataLoader multiprocessing issues
  pin_memory: true

# Model configuration (Prix Fixe defaults for yeast)
model:
  architecture: "dream_rnn"
  
  # Architecture hyperparameters
  hidden_dim: 320
  cnn_filters: 160  # Per CNN layer (320 total after concat)
  dropout_cnn: 0.2
  dropout_lstm: 0.5

# Training configuration
training:
  num_epochs: 80
  
  # Optimizer
  optimizer: "adamw"
  lr: 0.005         # For CNN layers
  lr_lstm: 0.001    # Lower LR for LSTM (attention-like components)
  weight_decay: 0.01
  
  # Scheduler
  scheduler: "onecycle"
  pct_start: 0.3  # 30% of training spent increasing LR
  
  # Loss function
  criterion: "mse"
  
  # Evaluation
  use_reverse_complement: true  # Average predictions from both orientations
  metric_for_best: "pearson_r"  # Save best model based on this metric
  early_stopping_patience: null  # null = no early stopping

# Output configuration
output:
  checkpoint_dir: "./checkpoints"
  results_dir: "./results"
  log_dir: "./logs"
  
  # What to save
  save_best_model: true
  save_final_model: true
  save_training_history: true
  save_predictions: false

# Hardware configuration
hardware:
  device: "cuda"
  device_id: 0
  deterministic: true
  benchmark: false
